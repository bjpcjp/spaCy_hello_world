{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy: large-scale Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* __Tokenization__:\t\n",
    "    Segmenting text into words, punctuations marks etc.\n",
    "* __Part-of-speech (POS) Tagging__:\t\n",
    "    Assigning word types to tokens, like verb or noun.\n",
    "* __Dependency Parsing__:\t\n",
    "    Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "* __Lemmatization__:\t\n",
    "    Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n",
    "* __Sentence Boundary Detection (SBD)__:\t\n",
    "    Finding and segmenting individual sentences.\n",
    "* __Named Entity Recognition (NER)__:\t\n",
    "    Labelling named \"real-world\" objects, like persons, companies or locations.\n",
    "* __Similarity__:\t\n",
    "    Comparing words, text spans and documents and how similar they are to each other.\n",
    "* __Text Classification__:\t\n",
    "    Assigning categories or labels to a whole document, or parts of a document.\n",
    "* __Rule-based Matching__:\t\n",
    "    Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "* __Training__:\t\n",
    "    Updating and improving a statistical model's predictions.\n",
    "* __Serialization__:\t\n",
    "    Saving objects to files or byte strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Installed models (spaCy v2.0.5)\u001b[0m\r\n",
      "    /home/bjpcjp/miniconda3/lib/python3.6/site-packages/spacy\r\n",
      "\r\n",
      "    TYPE        NAME                  MODEL                 VERSION                                   \r\n",
      "    package     en-core-web-sm        en_core_web_sm        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n",
      "    package     en-core-web-lg        en_core_web_lg        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n",
      "    package     de-core-news-sm       de_core_news_sm       \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n",
      "    link        en_core_web_lg        en_core_web_lg        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n",
      "    link        en                    en_core_web_sm        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n",
      "    link        de                    de_core_news_sm       \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m‚úî\u001b[0m      \r\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/bjpcjp/projects/nlp/spaCy, inifile:\n",
      "collected 0 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "\u001b[33m\u001b[1m========================= no tests ran in 0.02 seconds =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a statistical model - in this case, for English:\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# returns a language object, often named 'nlp'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a sample document.\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "# what tokens have been found?\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After tokenization, spaCy can parse and tag a Doc. The statistical model enables spaCy to predict which tag or label most likely applies in this context. \n",
    "* A model consists of binary data and is built by showing a system enough examples to make predictions that generalise across the language ‚Äì for example, a word following \"the\" in English is most likely a noun.\n",
    "* Linguistic annotations are available as __Token attributes__. spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Text__: The original word text.\n",
    "* __Lemma__: The base form of the word.\n",
    "* __POS__: The simple part-of-speech tag.\n",
    "* __Tag__: The detailed part-of-speech tag.\n",
    "* __Dep__: Syntactic dependency, i.e. the relation between tokens.\n",
    "* __Shape__: The word shape ‚Äì capitalisation, punctuation, digits.\n",
    "* __is alpha__: Is the token an alpha character?\n",
    "* __is stop__: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dependencies\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"930\" height=\"257.0\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,42.0 205.0,42.0 205.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M150,122.0 C150,82.0 200.0,82.0 200.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,124.0 L142,112.0 158,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M230,122.0 C230,82.0 280.0,82.0 280.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M280.0,124.0 L288.0,112.0 272.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M310,122.0 C310,82.0 360.0,82.0 360.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M360.0,124.0 L368.0,112.0 352.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M470,122.0 C470,82.0 520.0,82.0 520.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,124.0 L462,112.0 478,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M390,122.0 C390,42.0 525.0,42.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,124.0 L533.0,112.0 517.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M390,122.0 C390,2.0 610.0,2.0 610.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610.0,124.0 L618.0,112.0 602.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M710,122.0 C710,42.0 845.0,42.0 845.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M710,124.0 L702,112.0 718,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M790,122.0 C790,82.0 840.0,82.0 840.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,124.0 L782,112.0 798,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M630,122.0 C630,2.0 850.0,2.0 850.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,124.0 L858.0,112.0 842.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Named Entities ###\n",
    "\n",
    "* A named entity is a \"real-world object\" that's assigned a name ‚Äì a person, a country, a product or a book title. \n",
    "* spaCy can recognise various types of named entities in a document by asking the model for a prediction. \n",
    "* Because models are statistical and strongly depend on their training examples, this doesn't always work perfectly and might need some tuning depending on your use case.\n",
    "* Named entities are available as the ents property of a Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple \t 0 \t 5 \t ORG \t\n",
      "U.K. \t 27 \t 31 \t GPE \t\n",
      "$1 billion \t 44 \t 54 \t MONEY \t\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \"\\t\",         # original entity text\n",
    "          ent.start_char, \"\\t\",   # index of entity's start\n",
    "          ent.end_char, \"\\t\",     # index of entity's end\n",
    "          ent.label_, \"\\t\")       # entity label, ie. type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity\n",
    "\n",
    "* spaCy can compare two objects & predict their similarity. This is useful for building recommendation systems or flagging duplicates. For example, you can suggest content that's similar to what a user is currently viewing, or label a support ticket as a duplicate if it's very similar to an already existing one.\n",
    "* Each Doc, Span and Token comes with a __.similarity()__ method. Of course similarity is always subjective ‚Äì whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. spaCy's similarity model usually assumes a pretty general-purpose definition of similarity.\n",
    "* Similarity is found by comparing word vectors or \"word embeddings\", multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "1.0\n",
      "0.53906965\n",
      "0.28761008\n",
      "cat\n",
      "0.53906965\n",
      "1.0000001\n",
      "0.48752162\n",
      "banana\n",
      "0.28761008\n",
      "0.48752162\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    print(token1)\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6123773023244291\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'the fries were gross.')\n",
    "doc2 = nlp(u'worst fries ever.')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make them compact and fast, spaCy's small models (all packages that end in __sm__) don't ship with word vectors, and only include context-sensitive tensors. \n",
    "* This means you can still use the similarity() methods to compare documents, spans and tokens ‚Äì but the result won't be as good, and individual tokens won't have any vectors assigned. __So in order to use real word vectors, you need to download a larger model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    3% |‚ñà‚ñè                              | 31.9MB 494kB/s eta 0:27:405^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/site-packages/spacy/__main__.py\", line 31, in <module>\n",
      "    plac.call(commands[command])\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 33, in download\n",
      "    v=version))\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 89, in download_model\n",
      "    download_url], env=os.environ.copy())\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/subprocess.py\", line 269, in call\n",
      "    return p.wait(timeout=timeout)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/subprocess.py\", line 1457, in wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/bjpcjp/miniconda3/lib/python3.6/subprocess.py\", line 1404, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp(u'dog cat banana sasquatch')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text,         # Text: The original token text.\n",
    "          token.has_vector,   # has_vector: Does the token have a vector representation?\n",
    "          token.vector_norm,  # Vector_norm: The L2 norm of the token's vector (square root(sum of the values squared))\n",
    "          token.is_oov        # is_OOV: Is the word out-of-vocabulary?\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "![example](pipeline.png)\n",
    "\n",
    "* __Tokenizer__: creates Doc; segments text into tokens.\n",
    "* __Tagger__: creates Doc[i].tag; assigns part-of-speech tags.\n",
    "* __Parser__: creates dependency labels (head, dep, sents, noun_chunks)\n",
    "* __Ner__: creates .ents, .ent_iob, .ent_type; detects/labels named entities.\n",
    "* __Textcat__: creates .cats; assigns document labels.\n",
    "* __...__: assigns custom attributes/methods/properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab, hashes & lexemes\n",
    "\n",
    "* spaCy tries to store data in a vocabulary, the Vocab , that will be shared by multiple documents. \n",
    "* To save memory, spaCy also encodes all strings to hash values. Example: \"coffee\" = hash 3197928453018144401. \n",
    "* Entity labels like \"ORG\" and part-of-speech tags like \"VERB\" are also encoded. \n",
    "* Internally, spaCy only \"speaks\" in hash values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you process lots of documents containing the word \"coffee\" in many  contexts, storing the exact string \"coffee\" every time would take up way too much space. \n",
    "* spaCy instead hashes the string and stores it in the StringStore. Think of StringStore as a 2-way lookup table ‚Äì you can look up a string to get its hash, or a hash to get its string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I like coffee')\n",
    "\n",
    "assert doc.vocab.strings[u'coffee']           == 3197928453018144401\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text,       # original text\n",
    "          lexeme.orth,       # hash value\n",
    "          lexeme.shape_,     # abstract word shape\n",
    "          lexeme.prefix_,    # 1st letter of word string\n",
    "          lexeme.suffix_,    # last 3 letters of word string\n",
    "          lexeme.is_alpha,   # consists of alpha characters?\n",
    "          lexeme.is_digit,   # consitss of digits?\n",
    "          lexeme.is_title, \n",
    "          lexeme.lang_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hashes cannot be reversed - there's no way to resolve 3197928453018144401 back to \"coffee\". \n",
    "* All spaCy can do is look it up in the vocabulary. That's why you always need to make sure all objects you create __have access to the same vocabulary__. If they don't, spaCy might not be able to find the strings it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "doc = nlp(u'I like coffee') # original Doc\n",
    "assert doc.vocab.strings[u'coffee']           == 3197928453018144401\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee'\n",
    "\n",
    "empty_doc = Doc(Vocab()) # new Doc with empty Vocab\n",
    "\n",
    "empty_doc.vocab.strings.add(u'coffee') # add \"coffee\" and generate hash\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee' #\n",
    "\n",
    "new_doc = Doc(doc.vocab) # create new doc with first doc's vocab\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization\n",
    "\n",
    "* If modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you'll want to save your progress.\n",
    "* This means you'll have to translate its contents and structure into a format that can be saved. This process is called __serialization__. \n",
    "* spaCy comes with built-in serialization methods and supports the Pickle protocol.\n",
    "\n",
    "\n",
    "* __to_bytes__: returns bytes; example: nlp.to_bytes()\n",
    "* __from_bytes__: returns object: example: nlp.from_bytes(bytes)\n",
    "* __to_disk__: returns --: example: nlp.to_disk('/path')\n",
    "* __from_disk__: returns object: example: nlp.from_disk('/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_quotes = open('3quotes.txt','r').read()\n",
    "doc  = nlp(three_quotes)\n",
    "#doc.to_disk('/3quotes.bin')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "*  spaCy models are statistical. Every \"decision\" they make is a prediction that is based on the examples the model has seen during training. \n",
    "* To train a model, you need training data ‚Äì examples of text, and corresponding labels.\n",
    "* The model is then shown unlabelled text and will make a prediction. Because we know the correct answer, we can give the model feedback in the form of an __error gradient__ of the loss. \n",
    "* It calculates the difference between the training example and the expected output. The greater the difference, the more significant the gradient and the updates to our model.\n",
    "\n",
    "![training](training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Data\n",
    "\n",
    "* Every language is full of exceptions and special cases, especially amongst the most common words. Some exceptions are shared across languages, while others are entirely specific ‚Äì usually so specific that they need to be hard-coded. \n",
    "* The __lang__ module contains all language-specific data in simple Python files. This makes the data easy to update and extend.\n",
    "* The shared language data in the directory root includes rules that can be generalised across languages (basic punctuation, emoji, emoticons, single-letter abbreviations and norms for equivalent tokens with different spellings, like \" and ‚Äù, etc.) This helps the models make more accurate predictions. The individual language data in a submodule contains rules that are only relevant to a particular language. It also takes care of putting together all components and creating the Language subclass ‚Äì for example, English or German.\n",
    "\n",
    "![language](language.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Tour:\n",
    "* Install models & process text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Hello, world. Here are two sentences.')\n",
    "\n",
    "nlp_de = spacy.load('de')\n",
    "doc_de = nlp_de(u'Ich bin ein Berliner.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get tokens, noun chunks & sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant üçë \")\n",
    "\n",
    "assert doc[0].text == u'Peach'\n",
    "assert doc[1].text == u'emoji'\n",
    "assert doc[-1].text == u'üçë'\n",
    "assert doc[17:19].text == u'outranking eggplant'\n",
    "assert list(doc.noun_chunks)[0].text == u'Peach emoji'\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "assert sentences[1].text == u'Peach is the superior emoji.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get Part-Of-Text tags & flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "apple = doc[0]\n",
    "\n",
    "#assert [apple.pos_, apple.pos] == [u'PROPN', 17049293600679659579]\n",
    "#assert [apple.tag_, apple.tag] == [u'NNP', 15794550382381185553]\n",
    "#assert [apple.shape_, apple.shape] == [u'Xxxxx', 16072095006890171862]\n",
    "assert apple.is_alpha == True\n",
    "assert apple.is_punct == False\n",
    "\n",
    "billion = doc[10]\n",
    "assert billion.is_digit == False\n",
    "assert billion.like_num == True\n",
    "assert billion.like_email == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use hash values for any string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[u'coffee'] # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash] # 'coffee'\n",
    "\n",
    "assert doc[2].orth == coffee_hash == 3197928453018144401\n",
    "assert doc[2].text == coffee_text == u'coffee'\n",
    "\n",
    "beer_hash = doc.vocab.strings.add(u'beer') # 3073001599257881079\n",
    "beer_text = doc.vocab.strings[beer_hash] # 'beer'\n",
    "\n",
    "unicorn_hash = doc.vocab.strings.add(u'ü¶Ñ ') # 18234233413267120783\n",
    "unicorn_text = doc.vocab.strings[unicorn_hash] # 'ü¶Ñ '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recognize named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "ents = [(\n",
    "    ent.text, \n",
    "    ent.start_char, \n",
    "    ent.end_char, \n",
    "    ent.label_\n",
    ") for ent in doc.ents]\n",
    "\n",
    "assert ents == [(u'San Francisco', 0, 13, u'GPE')]\n",
    "\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(u'Netflix is hiring a new VP of global policy')\n",
    "\n",
    "doc.ents = [Span(\n",
    "    doc, 0, 1, \n",
    "    label=doc.vocab.strings[u'ORG'])]\n",
    "\n",
    "ents = [(\n",
    "    ent.start_char, \n",
    "    ent.end_char, \n",
    "    ent.label_\n",
    ") for ent in doc.ents]\n",
    "\n",
    "assert ents == [(0, 7, u'ORG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "train_data = [\n",
    "    (\"Uber blew through $1 million\", \n",
    "     {'entities': [(0, 4, 'ORG')]})]\n",
    "\n",
    "with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations] sgd=optimizer)\n",
    "    nlp.to_disk('/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
