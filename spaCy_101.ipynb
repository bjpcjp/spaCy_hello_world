{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy: large-scale Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* __Tokenization__:\t\n",
    "    Segmenting text into words, punctuations marks etc.\n",
    "* __Part-of-speech (POS) Tagging__:\t\n",
    "    Assigning word types to tokens, like verb or noun.\n",
    "* __Dependency Parsing__:\t\n",
    "    Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "* __Lemmatization__:\t\n",
    "    Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n",
    "* __Sentence Boundary Detection (SBD)__:\t\n",
    "    Finding and segmenting individual sentences.\n",
    "* __Named Entity Recognition (NER)__:\t\n",
    "    Labelling named \"real-world\" objects, like persons, companies or locations.\n",
    "* __Similarity__:\t\n",
    "    Comparing words, text spans and documents and how similar they are to each other.\n",
    "* __Text Classification__:\t\n",
    "    Assigning categories or labels to a whole document, or parts of a document.\n",
    "* __Rule-based Matching__:\t\n",
    "    Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "* __Training__:\t\n",
    "    Updating and improving a statistical model's predictions.\n",
    "* __Serialization__:\t\n",
    "    Saving objects to files or byte strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "#### [Prodigy: label annotation tool](https://prodi.gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Installed models (spaCy v2.0.5)\u001b[0m\n",
      "    /home/bjpcjp/miniconda3/lib/python3.6/site-packages/spacy\n",
      "\n",
      "    TYPE        NAME                  MODEL                 VERSION                                   \n",
      "    package     en-core-web-sm        en_core_web_sm        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n",
      "    package     en-core-web-lg        en_core_web_lg        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n",
      "    package     de-core-news-sm       de_core_news_sm       \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n",
      "    link        en_core_web_lg        en_core_web_lg        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n",
      "    link        en                    en_core_web_sm        \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n",
      "    link        de                    de_core_news_sm       \u001b[38;5;2m2.0.0\u001b[0m    \u001b[38;5;2m✔\u001b[0m      \n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\r\n",
      "rootdir: /home/bjpcjp/projects/nlp/spaCy, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 0 items                                                              \u001b[0m\u001b[1m\r",
      "collected 0 items                                                               \u001b[0m\r\n",
      "\r\n",
      "\u001b[33m\u001b[1m========================= no tests ran in 0.00 seconds =========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a statistical model - in this case, for English:\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# returns a language object, often named 'nlp'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a sample document.\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "# what tokens have been found?\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After tokenization, spaCy can parse and tag a Doc. The statistical model enables spaCy to predict which tag or label most likely applies in this context. \n",
    "* A model consists of binary data and is built by showing a system enough examples to make predictions that generalise across the language – for example, a word following \"the\" in English is most likely a noun.\n",
    "* Linguistic annotations are available as __Token attributes__. spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Text__: The original word text.\n",
    "* __Lemma__: The base form of the word.\n",
    "* __POS__: The simple part-of-speech tag.\n",
    "* __Tag__: The detailed part-of-speech tag.\n",
    "* __Dep__: Syntactic dependency, i.e. the relation between tokens.\n",
    "* __Shape__: The word shape – capitalisation, punctuation, digits.\n",
    "* __is alpha__: Is the token an alpha character?\n",
    "* __is stop__: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dependencies\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"930\" height=\"257.0\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,42.0 205.0,42.0 205.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M150,122.0 C150,82.0 200.0,82.0 200.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,124.0 L142,112.0 158,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M230,122.0 C230,82.0 280.0,82.0 280.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M280.0,124.0 L288.0,112.0 272.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M310,122.0 C310,82.0 360.0,82.0 360.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M360.0,124.0 L368.0,112.0 352.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M470,122.0 C470,82.0 520.0,82.0 520.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,124.0 L462,112.0 478,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M390,122.0 C390,42.0 525.0,42.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,124.0 L533.0,112.0 517.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M390,122.0 C390,2.0 610.0,2.0 610.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610.0,124.0 L618.0,112.0 602.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M710,122.0 C710,42.0 845.0,42.0 845.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M710,124.0 L702,112.0 718,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M790,122.0 C790,82.0 840.0,82.0 840.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,124.0 L782,112.0 798,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M630,122.0 C630,2.0 850.0,2.0 850.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,124.0 L858.0,112.0 842.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Named Entities ###\n",
    "\n",
    "* A named entity is a \"real-world object\" that's assigned a name – a person, a country, a product or a book title. \n",
    "* spaCy can recognise various types of named entities in a document by asking the model for a prediction. \n",
    "* Because models are statistical and strongly depend on their training examples, this doesn't always work perfectly and might need some tuning depending on your use case.\n",
    "* Named entities are available as the ents property of a Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple \t 0 \t 5 \t ORG \t\n",
      "U.K. \t 27 \t 31 \t GPE \t\n",
      "$1 billion \t 44 \t 54 \t MONEY \t\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \"\\t\",         # original entity text\n",
    "          ent.start_char, \"\\t\",   # index of entity's start\n",
    "          ent.end_char, \"\\t\",     # index of entity's end\n",
    "          ent.label_, \"\\t\")       # entity label, ie. type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity\n",
    "\n",
    "* spaCy can compare two objects & predict their similarity. This is useful for building recommendation systems or flagging duplicates. For example, you can suggest content that's similar to what a user is currently viewing, or label a support ticket as a duplicate if it's very similar to an already existing one.\n",
    "* Each Doc, Span and Token comes with a __.similarity()__ method. Of course similarity is always subjective – whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. spaCy's similarity model usually assumes a pretty general-purpose definition of similarity.\n",
    "* Similarity is found by comparing word vectors or \"word embeddings\", multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "1.0\n",
      "0.53906965\n",
      "0.28761008\n",
      "cat\n",
      "0.53906965\n",
      "1.0000001\n",
      "0.48752162\n",
      "banana\n",
      "0.28761008\n",
      "0.48752162\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    print(token1)\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6123773023244291\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'the fries were gross.')\n",
    "doc2 = nlp(u'worst fries ever.')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make them compact and fast, spaCy's small models (all packages that end in __sm__) don't ship with word vectors, and only include context-sensitive tensors. \n",
    "* This means you can still use the similarity() methods to compare documents, spans and tokens – but the result won't be as good, and individual tokens won't have any vectors assigned. __So in order to use real word vectors, you need to download a larger model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "sasquatch True 6.9789977 False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp(u'dog cat banana sasquatch')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text,         # Text: The original token text.\n",
    "          token.has_vector,   # has_vector: Does the token have a vector representation?\n",
    "          token.vector_norm,  # Vector_norm: The L2 norm of the token's vector (square root(sum of the values squared))\n",
    "          token.is_oov        # is_OOV: Is the word out-of-vocabulary?\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "![example](pipeline.png)\n",
    "\n",
    "* __Tokenizer__: creates Doc; segments text into tokens.\n",
    "* __Tagger__: creates Doc[i].tag; assigns part-of-speech tags.\n",
    "* __Parser__: creates dependency labels (head, dep, sents, noun_chunks)\n",
    "* __Ner__: creates .ents, .ent_iob, .ent_type; detects/labels named entities.\n",
    "* __Textcat__: creates .cats; assigns document labels.\n",
    "* __...__: assigns custom attributes/methods/properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab, hashes & lexemes\n",
    "\n",
    "* spaCy tries to store data in a vocabulary, the Vocab , that will be shared by multiple documents. \n",
    "* To save memory, spaCy also encodes all strings to hash values. Example: \"coffee\" = hash 3197928453018144401. \n",
    "* Entity labels like \"ORG\" and part-of-speech tags like \"VERB\" are also encoded. \n",
    "* Internally, spaCy only \"speaks\" in hash values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you process lots of documents containing the word \"coffee\" in many  contexts, storing the exact string \"coffee\" every time would take up way too much space. \n",
    "* spaCy instead hashes the string and stores it in the StringStore. Think of StringStore as a 2-way lookup table – you can look up a string to get its hash, or a hash to get its string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I like coffee')\n",
    "\n",
    "assert doc.vocab.strings[u'coffee']           == 3197928453018144401\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "like 18194338103975822726 xxxx l ike True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text,       # original text\n",
    "          lexeme.orth,       # hash value\n",
    "          lexeme.shape_,     # abstract word shape\n",
    "          lexeme.prefix_,    # 1st letter of word string\n",
    "          lexeme.suffix_,    # last 3 letters of word string\n",
    "          lexeme.is_alpha,   # consists of alpha characters?\n",
    "          lexeme.is_digit,   # consitss of digits?\n",
    "          lexeme.is_title, \n",
    "          lexeme.lang_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hashes cannot be reversed - there's no way to resolve 3197928453018144401 back to \"coffee\". \n",
    "* All spaCy can do is look it up in the vocabulary. That's why you always need to make sure all objects you create __have access to the same vocabulary__. If they don't, spaCy might not be able to find the strings it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "doc = nlp(u'I like coffee') # original Doc\n",
    "assert doc.vocab.strings[u'coffee']           == 3197928453018144401\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee'\n",
    "\n",
    "empty_doc = Doc(Vocab()) # new Doc with empty Vocab\n",
    "\n",
    "empty_doc.vocab.strings.add(u'coffee') # add \"coffee\" and generate hash\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee' #\n",
    "\n",
    "new_doc = Doc(doc.vocab) # create new doc with first doc's vocab\n",
    "assert doc.vocab.strings[3197928453018144401] == u'coffee' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization\n",
    "\n",
    "* If modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you'll want to save your progress.\n",
    "* This means you'll have to translate its contents and structure into a format that can be saved. This process is called __serialization__. \n",
    "* spaCy comes with built-in serialization methods and supports the Pickle protocol.\n",
    "\n",
    "\n",
    "* __to_bytes__: returns bytes; example: nlp.to_bytes()\n",
    "* __from_bytes__: returns object: example: nlp.from_bytes(bytes)\n",
    "* __to_disk__: returns --: example: nlp.to_disk('/path')\n",
    "* __from_disk__: returns object: example: nlp.from_disk('/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Four score and seven years ago\n",
       "You can't handle the truth\n",
       "You're going to need a bigger boat"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_quotes = open('3quotes.txt','r').read()\n",
    "doc  = nlp(three_quotes)\n",
    "\n",
    "byte_string = doc.to_bytes()\n",
    "open('3quotes.bin', 'wb').write(byte_string)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "*  spaCy models are statistical. Every \"decision\" they make is a prediction that is based on the examples the model has seen during training. \n",
    "* To train a model, you need training data – examples of text, and corresponding labels.\n",
    "* The model is then shown unlabelled text and will make a prediction. Because we know the correct answer, we can give the model feedback in the form of an __error gradient__ of the loss. \n",
    "* It calculates the difference between the training example and the expected output. The greater the difference, the more significant the gradient and the updates to our model.\n",
    "\n",
    "![training](training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training data should always represent the data we want to process. A model trained on Wikipedia, where sentences in the first person are extremely rare, will likely perform badly on Twitter. A model trained on romantic novels will likely perform badly on legal text.\n",
    "* This means that to know how the model is performing, you need training data __and__ evaluation data. If you only test the model with the data it was trained on, you'll have no idea how well it's generalising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Data\n",
    "\n",
    "* Every language is full of exceptions and special cases, especially amongst the most common words. Some exceptions are shared across languages, while others are entirely specific – usually so specific that they need to be hard-coded. \n",
    "* The __lang__ module contains all language-specific data in simple Python files. This makes the data easy to update and extend.\n",
    "* The shared language data in the directory root includes rules that can be generalised across languages (basic punctuation, emoji, emoticons, single-letter abbreviations and norms for equivalent tokens with different spellings, like \" and ”, etc.) This helps the models make more accurate predictions. The individual language data in a submodule contains rules that are only relevant to a particular language. It also takes care of putting together all components and creating the Language subclass – for example, English or German.\n",
    "\n",
    "![language](language.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Tour:\n",
    "### Install models & process text (use %%capture to mute output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Hello, world. Here are two sentences.')\n",
    "\n",
    "nlp_de = spacy.load('de')\n",
    "doc_de = nlp_de(u'Ich bin ein Berliner.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Get tokens, noun chunks & sentences](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant 🍑 \")\n",
    "\n",
    "# words found correctly?\n",
    "assert doc[0].text == u'Peach'\n",
    "assert doc[1].text == u'emoji'\n",
    "assert doc[-1].text == u'🍑'\n",
    "assert doc[17:19].text == u'outranking eggplant'\n",
    "assert list(doc.noun_chunks)[0].text == u'Peach emoji'\n",
    "\n",
    "# 3 sentences total, correct?\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "\n",
    "# 2nd sentence found correctly?\n",
    "assert sentences[1].text == u'Peach is the superior emoji.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Get Part-Of-Text tags & flags](https://spacy.io/usage/linguistic-features#pos-tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN 95\n",
      "NNP 15794550382381185553\n",
      "Xxxxx 16072095006890171862\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "apple = doc[0]\n",
    "\n",
    "print(apple.pos_, apple.pos)\n",
    "print(apple.tag_, apple.tag)\n",
    "print(apple.shape_, apple.shape)\n",
    "\n",
    "assert [apple.pos_, apple.pos] == [u'PROPN', 95]\n",
    "assert [apple.tag_, apple.tag] == [u'NNP', 15794550382381185553]\n",
    "assert [apple.shape_, apple.shape] == [u'Xxxxx', 16072095006890171862]\n",
    "assert apple.is_alpha == True\n",
    "assert apple.is_punct == False\n",
    "\n",
    "billion = doc[10]\n",
    "assert billion.is_digit   == False\n",
    "assert billion.like_num   == True\n",
    "assert billion.like_email == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Use hash values for any string](https://spacy.io/usage/spacy-101#vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[u'coffee'] # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash] # 'coffee'\n",
    "\n",
    "assert doc[2].orth == coffee_hash == 3197928453018144401\n",
    "assert doc[2].text == coffee_text == u'coffee'\n",
    "\n",
    "beer_hash = doc.vocab.strings.add(u'beer') # 3073001599257881079\n",
    "beer_text = doc.vocab.strings[beer_hash] # 'beer'\n",
    "\n",
    "unicorn_hash = doc.vocab.strings.add(u'🦄 ') # 18234233413267120783\n",
    "unicorn_text = doc.vocab.strings[unicorn_hash] # '🦄 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Recognize named entities](https://spacy.io/usage/linguistic-features#named-entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "[(0, 7, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "\n",
    "ents = [(\n",
    "    ent.text, \n",
    "    ent.start_char, \n",
    "    ent.end_char, \n",
    "    ent.label_       # labels are pretty abstract & language-specific. See explain().\n",
    ") for ent in doc.ents]\n",
    "\n",
    "assert ents == [(u'San Francisco', 0, 13, u'GPE')]\n",
    "\n",
    "print(spacy.explain(u'GPE'))\n",
    "\n",
    "# Span is a class - a slice of a Doc object.\n",
    "\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(u'Netflix is hiring a new VP of global policy')\n",
    "\n",
    "doc.ents = [Span(\n",
    "    doc, 0, 1, \n",
    "    label=doc.vocab.strings[u'ORG'])]\n",
    "\n",
    "ents = [(\n",
    "    ent.start_char, \n",
    "    ent.end_char, \n",
    "    ent.label_\n",
    ") for ent in doc.ents]\n",
    "\n",
    "print(ents)\n",
    "\n",
    "assert ents == [(0, 7, u'ORG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Train neural net models](https://spacy.io/usage/training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "train_data = [\n",
    "    (\"Uber blew through $1 million\", \n",
    "     {'entities': [(0, 4, 'ORG')]})]\n",
    "\n",
    "\n",
    "with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):\n",
    "\n",
    "    # begin_training(): return optimizer function to update model's weights.\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "\n",
    "        # shuffle to ensure training doesn't generalize on order of examples.\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "    open('uber_1m_test.bin', 'wb').write(nlp.to_bytes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat training with dropout rate (0.25 = each feature has 1/4 chance of being ignored.)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "train_data = [\n",
    "    (\"Uber blew through $1 million\", \n",
    "     {'entities': [(0, 4, 'ORG')]})]\n",
    "\n",
    "\n",
    "with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):\n",
    "\n",
    "    # begin_training(): return optimizer function to update model's weights.\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "\n",
    "        # shuffle to ensure training doesn't generalize on order of examples.\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], drop=0.25, sgd=optimizer)\n",
    "    open('uber_1m_test_with_dropout.bin', 'wb').write(nlp.to_bytes())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls uber*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Visualize](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc_dep = nlp(u'This is a sentence.')\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "              u'in 2007, few people outside of the company took him seriously.')\n",
    "\n",
    "displacy.render(doc_ent, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/miniconda3/lib/python3.6/runpy.py:193: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"750\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc_dep, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Get word vectors & similarity](https://spacy.io/usage/vectors-similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36872467 0.40738952\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple  = doc[0]\n",
    "banana = doc[2]\n",
    "pasta  = doc[6]\n",
    "hippo  = doc[8]\n",
    "\n",
    "assert apple.similarity(banana) < pasta.similarity(hippo) # fails on apple|banana > pasta|hippo?\n",
    "\n",
    "assert apple.has_vector\n",
    "assert banana.has_vector\n",
    "assert pasta.has_vector\n",
    "assert hippo.has_vector\n",
    "\n",
    "print(apple.similarity(banana), pasta.similarity(hippo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
